import os

import streamlit as st
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
import openai

from dotenv import load_dotenv
load_dotenv()

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã®æŒ‡å®š
index_name = "index1"
# Cognitive Searchã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨APIã‚­ãƒ¼ã‚’æŒ‡å®š
endpoint = os.environ["AZURE_SEARCH_ENDPOINT"]
key = os.environ["AZURE_SEARCH_API_KEY"]

# OpenAIã®APIã‚­ãƒ¼ã‚’è¨­å®š 
openai.api_type = 'azure'
openai.api_base = os.environ["AZURE_OPENAI_ENDPOINT"]
openai.api_key = os.environ["AZURE_OPENAI_API_KEY"]
emb_model_name = "text-embedding-3-large"
llm_model_name = "gpt-4o"
openai.api_version = "2024-02-01"

search_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))

# Embeddingã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_embedding(text):
    response = openai.Embedding.create(input=text, engine=emb_model_name)
    return response["data"][0]["embedding"]

# Azure Cognitive Searchã‹ã‚‰çµæœã‚’å–å¾—ã™ã‚‹é–¢æ•°
def cognitive_search(query, k=3):
    embedding = get_embedding(query)
    results = search_client.search(
        search_text="",
        vectors=[{
            "value": embedding,
            "k": k,
            "fields": "text_vector"
        }]
    )
    documents = []
    for result in results:
        documents.append(result["text"])
    return documents

# LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã—ã¦å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def generate_response(question, context_docs):
    context = "\n\n".join(context_docs)
    prompt = f"""
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{context}
"""
    completion = openai.ChatCompletion.create(
        engine=llm_model_name,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": question},
        ],
        temperature=0.2,
    )
    return completion.choices[0].message.content.strip()

# Streamlit UIã®æ§‹ç¯‰
st.title("ğŸ“š Azure RAG Chatbot")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å–å¾—
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # æ¤œç´¢çµæœã‚’å–å¾—
    with st.spinner("æ¤œç´¢ä¸­..."):
        docs = cognitive_search(prompt)

    # LLMã‹ã‚‰ã®å›ç­”ã‚’ç”Ÿæˆ
    with st.spinner("å›ç­”ç”Ÿæˆä¸­..."):
        answer = generate_response(prompt, docs)

    # å›ç­”ã‚’è¡¨ç¤º
    with st.chat_message("assistant"):
        st.markdown(answer)
    st.session_state.messages.append({"role": "assistant", "content": answer})
